# @package store
# Unified pgai configuration for MemFuse
# This file consolidates all pgai-related settings to eliminate duplication

# =============================================================================
# CORE PGAI BACKEND CONFIGURATION
# =============================================================================

# Primary backend selection
backend: "pgai"

# =============================================================================
# DATABASE CONNECTION SETTINGS
# =============================================================================

# PostgreSQL connection (inherited from database config but can be overridden)
database:
  host: ${oc.env:POSTGRES_HOST,localhost}
  port: ${oc.env:POSTGRES_PORT,5432}
  database: ${oc.env:POSTGRES_DB,memfuse}
  user: ${oc.env:POSTGRES_USER,postgres}
  password: ${oc.env:POSTGRES_PASSWORD,postgres}
  
  # Connection pool settings (unified across all pgai usage)
  pool_size: 10
  max_overflow: 20
  pool_timeout: 30.0
  pool_recycle: 3600

# =============================================================================
# PGAI-SPECIFIC SETTINGS
# =============================================================================

pgai:
  # Core pgai features
  enabled: true
  vectorizer_worker_enabled: true
  auto_embedding: true
  auto_initialize: true

  # Event-driven configuration (enable real-time trigger mode)
  immediate_trigger: true
  
  # Embedding configuration
  embedding:
    model: "text-embedding-3-small"
    dimensions: 1536
    batch_size: 100
    
  # Chunking configuration
  chunking:
    chunk_size: 1000
    chunk_overlap: 200
    
  # Performance settings
  performance:
    max_retries: 3
    retry_delay: 5.0
    timeout: 30.0
    enable_transactions: true

  # Event-driven specific settings
  event_driven:
    worker_count: 2              # Number of concurrent processing workers
    queue_size: 100              # Event queue size
    enable_metrics: true         # Enable performance metrics
    notification_timeout: 10.0   # Notification timeout in seconds

# =============================================================================
# TABLE AND VIEW NAMING CONVENTIONS
# =============================================================================

# Unified table naming for different memory layers
tables:
  # M0 layer tables
  m0:
    messages: "m0_episodic"
    metadata: "m0_metadata"
    embedding_view: "m0_episodic_embedding"
    vectorizer_name: "m0_episodic_vectorizer"
    
  # M1 layer tables (semantic memory implementation)
  m1:
    facts: "m1_semantic"
    lineage: "m1_lineage"
    conflicts: "m1_conflicts"
    embedding_view: "m1_semantic_embedding"
    vectorizer_name: "m1_semantic_vectorizer"
    


# =============================================================================
# STORE-SPECIFIC SETTINGS
# =============================================================================

# Store interface settings
store:
  buffer_size: 10
  cache_size: 100
  
  # Query defaults
  top_k: 5
  similarity_threshold: 0.3
  
  # Multi-path retrieval configuration
  multi_path:
    keyword_weight: 0.2
    vector_weight: 0.5
    graph_weight: 0.3
    use_keyword: true
    use_vector: true
    use_graph: false
    fusion_strategy: "rrf"

# =============================================================================
# MEMORY LAYER INTEGRATION (Enhanced for Dual-Layer PgAI)
# =============================================================================

# Memory layer specific configurations with PgAI integration
memory_layers:
  # M0 layer configuration (Episodic Memory)
  m0:
    enabled: true
    priority: 1
    table_name: "m0_episodic"
    
    # PgAI-specific settings for M0
    pgai:
      auto_embedding: true
      immediate_trigger: true
      embedding_model: "all-MiniLM-L6-v2"
      embedding_dimensions: 384
      
    # Processing pipeline configuration
    processing_pipeline: ["storage"]  # M0 only does storage
    
    # Performance settings
    performance:
      max_retries: 3
      retry_interval: 5.0
      worker_count: 2
      queue_size: 100
      batch_size: 10
      
    # Storage backend preferences
    storage_backends: ["vector", "keyword", "sql"]
    
  # M1 layer configuration (Semantic Memory)
  m1:
    enabled: true
    priority: 2
    table_name: "m1_semantic"
    
    # PgAI-specific settings for M1
    pgai:
      auto_embedding: true
      immediate_trigger: true
      embedding_model: "all-MiniLM-L6-v2"
      embedding_dimensions: 384
      
    # Processing pipeline configuration
    processing_pipeline: ["fact_extraction", "storage"]  # M1 does fact extraction then storage
    
    # Fact extraction configuration
    fact_extraction:
      enabled: true
      llm_model: "grok-3-mini"
      temperature: 0.3
      max_tokens: 1000
      max_facts_per_chunk: 10
      min_confidence_threshold: 0.7
      batch_size: 5
      context_window: 2  # chunks before/after for context

      # Flexible classification system
      classification_strategy: "open"  # "open", "predefined", "custom"
      enable_auto_categorization: true
      custom_fact_types: []  # Can be extended as needed
      
    # Performance settings
    performance:
      max_retries: 3
      retry_interval: 5.0
      worker_count: 2
      queue_size: 100
      batch_size: 5  # Smaller batch for LLM processing
      
    # Storage backend preferences
    storage_backends: ["vector", "sql"]  # M1 primarily uses vector and SQL
    
    # Async processing settings (to avoid blocking M0)
    async_processing:
      enabled: true
      queue_timeout: 30.0
      max_concurrent_extractions: 3
      fallback_on_failure: true  # Continue M0 processing if M1 fails
    


# =============================================================================
# STORAGE BACKEND CONFIGURATION
# =============================================================================

# Unified storage backend settings for pgai
storage_backends:
  vector:
    connection_pool_size: 5
    timeout: 30.0
    batch_size: 100
    store_type: "pgai"
    enable_transactions: true
    backend: "pgai"
    
  keyword:
    connection_pool_size: 3
    timeout: 15.0
    enable_transactions: true
    backend: "sqlite"  # Keyword search still uses SQLite for performance
    
  graph:
    connection_pool_size: 3
    timeout: 20.0
    enable_auto_save: true
    store_type: "networkx"
    enable_transactions: true
    backend: "sqlite"  # Graph storage uses SQLite for simplicity
    
  sql:
    connection_pool_size: 5
    timeout: 30.0
    enable_wal_mode: true
    enable_transactions: true
    backend: "pgai"  # SQL operations use pgai/PostgreSQL

# =============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# =============================================================================

# Development environment settings
development:
  pgai:
    vectorizer_worker_enabled: false  # Disable worker in dev for simplicity
    auto_embedding: false             # Manual embedding control in dev
  database:
    pool_size: 5                      # Smaller pool for dev
  performance:
    enable_debug_logging: true
    
# Production environment settings  
production:
  pgai:
    vectorizer_worker_enabled: true   # Enable worker in production
    auto_embedding: true              # Auto embedding in production
  database:
    pool_size: 20                     # Larger pool for production
  performance:
    enable_metrics_collection: true
    metrics_export_interval: 300

# =============================================================================
# MIGRATION AND COMPATIBILITY
# =============================================================================

# Settings for backward compatibility and migration
compatibility:
  legacy_table_names: false          # Use new unified naming convention
  auto_migrate_tables: true          # Automatically migrate old table structures
  preserve_old_data: true            # Keep old data during migration
