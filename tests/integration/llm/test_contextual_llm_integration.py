#!/usr/bin/env python3
"""
Test LLM API calling functionality, especially contextual chunking related features
Using XAI's grok-3-mini model
"""

import asyncio
import os
import sys
from pathlib import Path

# Add src path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "src"))

from memfuse_core.rag.chunk.base import ChunkData
from memfuse_core.rag.chunk.contextual import ContextualChunkStrategy
from memfuse_core.llm.base import LLMRequest, LLMResponse
from memfuse_core.llm.providers.openai import OpenAIProvider
from memfuse_core.llm.prompts.manager import get_prompt_manager


async def test_xai_llm_provider():
    """Test XAI LLM provider functionality"""
    print("ğŸ§  Testing XAI LLM provider functionality...")

    try:
        # Configuration - use XAI settings
        xai_api_key = os.getenv('XAI_API_KEY') or os.getenv('OPENAI_API_KEY')
        xai_base_url = os.getenv('XAI_API_URL') or os.getenv('OPENAI_BASE_URL')

        if not xai_api_key:
            print("âŒ XAI_API_KEY or OPENAI_API_KEY environment variable not found")
            return None

        print(f"ğŸ”‘ Using API Key: {xai_api_key[:10]}...")
        print(f"ğŸŒ Using Base URL: {xai_base_url}")

        config = {
            'api_key': xai_api_key,
            'base_url': xai_base_url,
            'model': 'grok-3-mini'
        }

        # Create OpenAI provider (compatible with XAI)
        provider = OpenAIProvider(config)

        # Test simple call
        request = LLMRequest(
            messages=[{"role": "user", "content": "Hello, respond with exactly 'LLM working!'"}],
            model="grok-3-mini",
            max_tokens=50,
            temperature=0.3
        )

        print("ğŸ“¡ Sending test request...")
        response = await provider.generate(request)

        if response.success:
            print(f"âœ… XAI LLM call successful: {response.content}")
            print(f"   Model: {response.model}")
            print(f"   Token usage: {response.usage.total_tokens}")
            return provider
        else:
            print(f"âŒ XAI LLM call failed: {response.error}")
            return None

    except Exception as e:
        print(f"âŒ XAI LLM initialization failed: {e}")
        return None


async def test_contextual_description_with_xai(llm_provider):
    """æµ‹è¯•ä½¿ç”¨XAIçš„contextual descriptionç”Ÿæˆ"""
    print("\nğŸ” æµ‹è¯•XAI Contextual Descriptionç”Ÿæˆ...")
    
    if not llm_provider:
        print("âŒ æ— æ³•æµ‹è¯• - LLMæä¾›è€…ä¸å¯ç”¨")
        return False
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        context_chunks = [
            ChunkData(
                content="[ASSISTANT]: Hi! How are you doing tonight?\n[USER]: I'm doing great. Just relaxing with my two dogs.",
                metadata={"chunk_id": "chunk_0", "session_id": "test"}
            ),
            ChunkData(
                content="[ASSISTANT]: Great. In my spare time I do volunteer work.\n[USER]: That's neat. What kind of volunteer work do you do?",
                metadata={"chunk_id": "chunk_1", "session_id": "test"}
            )
        ]
        
        current_chunk = "[ASSISTANT]: I work in a homeless shelter in my town.\n[USER]: Good for you. Do you like vintage cars? I've two older mustangs."
        
        print(f"ğŸ“„ å½“å‰chunk: {current_chunk[:50]}...")
        print(f"ğŸ”— ä¸Šä¸‹æ–‡chunks: {len(context_chunks)}")
        
        # ä½¿ç”¨promptç®¡ç†å™¨ç”Ÿæˆprompt
        prompt_manager = get_prompt_manager()
        
        past_messages = "\n\n--- Previous Chunk ---\n\n".join([
            chunk.content for chunk in context_chunks
        ])
        
        prompt = prompt_manager.get_prompt(
            "contextual_chunking",
            past_messages=past_messages,
            current_messages=current_chunk,
            chunk_content=current_chunk
        )
        
        print(f"ğŸ“ ç”Ÿæˆçš„prompté•¿åº¦: {len(prompt)}")
        
        # åˆ›å»ºLLMè¯·æ±‚ - ä½¿ç”¨grok-3-mini
        request = LLMRequest(
            messages=[{"role": "user", "content": prompt}],
            model="grok-3-mini",
            max_tokens=150,
            temperature=0.3
        )
        
        print("ğŸ“¡ å‘é€contextual descriptionè¯·æ±‚...")
        # ç”Ÿæˆå“åº”
        response = await llm_provider.generate(request)
        
        if response.success:
            print(f"âœ… XAI Contextual descriptionç”ŸæˆæˆåŠŸ:")
            print(f"   å†…å®¹: {response.content}")
            print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens}")
            return True
        else:
            print(f"âŒ XAI Contextual descriptionç”Ÿæˆå¤±è´¥: {response.error}")
            return False
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        return False


async def test_contextual_strategy_with_xai(llm_provider):
    """Test ContextualChunkStrategy integration with XAI"""
    print("\nğŸš€ Testing ContextualChunkStrategy integration with XAI...")

    if not llm_provider:
        print("âŒ Unable to test - LLM provider unavailable")
        return False

    try:
        # Create strategy instance - using grok-3-mini
        strategy = ContextualChunkStrategy(
            enable_contextual=True,
            context_window_size=2,
            llm_provider=llm_provider,
            gpt_model="grok-3-mini"
        )
        
        print(f"âœ… ç­–ç•¥åˆ›å»ºæˆåŠŸ:")
        print(f"   enable_contextual: {strategy.enable_contextual}")
        print(f"   context_window_size: {strategy.context_window_size}")
        print(f"   llm_provider: {strategy.llm_provider is not None}")
        print(f"   gpt_model: {strategy.gpt_model}")
        
        # æµ‹è¯•contextual descriptionç”Ÿæˆæ–¹æ³•
        context_chunks = [
            ChunkData(
                content="[ASSISTANT]: Hi! How are you doing tonight?\n[USER]: I'm doing great. Just relaxing with my two dogs.",
                metadata={"chunk_id": "chunk_0", "session_id": "test"}
            )
        ]
        
        current_chunk = "[ASSISTANT]: Great. In my spare time I do volunteer work.\n[USER]: That's neat. What kind of volunteer work do you do?"
        
        print(f"\nğŸ“„ æµ‹è¯•_generate_contextual_descriptionæ–¹æ³•...")
        print("ğŸ“¡ è°ƒç”¨XAI API...")
        
        description = await strategy._generate_contextual_description(
            current_chunk, context_chunks
        )
        
        print(f"âœ… XAIæ–¹æ³•è°ƒç”¨æˆåŠŸ:")
        print(f"   æè¿°: {description}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_create_enhanced_chunk_with_xai(llm_provider):
    """æµ‹è¯•ä½¿ç”¨XAIçš„_create_enhanced_chunk_asyncæ–¹æ³•"""
    print("\nğŸ”§ æµ‹è¯•XAI _create_enhanced_chunk_asyncæ–¹æ³•...")
    
    if not llm_provider:
        print("âŒ æ— æ³•æµ‹è¯• - LLMæä¾›è€…ä¸å¯ç”¨")
        return False
    
    try:
        # åˆ›å»ºç­–ç•¥å®ä¾‹
        strategy = ContextualChunkStrategy(
            enable_contextual=True,
            context_window_size=2,
            llm_provider=llm_provider,
            gpt_model="grok-3-mini"
        )
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        previous_chunks = [
            ChunkData(
                content="[ASSISTANT]: Hi! How are you doing tonight?\n[USER]: I'm doing great. Just relaxing with my two dogs.",
                metadata={"chunk_id": "chunk_0", "session_id": "test"}
            )
        ]
        
        chunk_content = "[ASSISTANT]: Great. In my spare time I do volunteer work.\n[USER]: That's neat. What kind of volunteer work do you do?"
        
        print(f"ğŸ“„ æµ‹è¯•chunkå†…å®¹: {chunk_content[:50]}...")
        print(f"ğŸ”— å†å²chunks: {len(previous_chunks)}")
        print("ğŸ“¡ è°ƒç”¨XAI APIç”Ÿæˆenhanced chunk...")
        
        # è°ƒç”¨æ–¹æ³•
        enhanced_chunk = await strategy._create_enhanced_chunk_async(
            chunk_content, 0, previous_chunks, "test_session"
        )
        
        print(f"âœ… XAI Enhanced chunkåˆ›å»ºæˆåŠŸ:")
        print(f"   has_context: {enhanced_chunk.metadata.get('has_context')}")
        print(f"   gpt_enhanced: {enhanced_chunk.metadata.get('gpt_enhanced')}")
        print(f"   context_window_size: {enhanced_chunk.metadata.get('context_window_size')}")
        print(f"   context_chunk_ids: {enhanced_chunk.metadata.get('context_chunk_ids')}")
        
        if enhanced_chunk.metadata.get('contextual_description'):
            desc = enhanced_chunk.metadata['contextual_description']
            print(f"   contextual_description: {desc}")
        
        return True
        
    except Exception as e:
        print(f"âŒ XAI Enhanced chunkåˆ›å»ºå¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹XAI LLM Contextual Chunkingå®Œæ•´æµ‹è¯•\n")
    print(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"ğŸ” æ£€æŸ¥ç¯å¢ƒå˜é‡...")
    print(f"   XAI_API_KEY: {'å·²è®¾ç½®' if os.getenv('XAI_API_KEY') else 'æœªè®¾ç½®'}")
    print(f"   OPENAI_API_KEY: {'å·²è®¾ç½®' if os.getenv('OPENAI_API_KEY') else 'æœªè®¾ç½®'}")
    print(f"   XAI_API_URL: {os.getenv('XAI_API_URL', 'æœªè®¾ç½®')}")
    print()
    
    # 1. æµ‹è¯•XAI LLMæä¾›è€…
    llm_provider = await test_xai_llm_provider()
    
    # 2. æµ‹è¯•promptç®¡ç†å™¨
    try:
        prompt_manager = get_prompt_manager()
        prompt_success = True
        print("âœ… Promptç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        prompt_success = False
        print(f"âŒ Promptç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # 3. æµ‹è¯•XAIç›¸å…³åŠŸèƒ½
    if llm_provider:
        description_success = await test_contextual_description_with_xai(llm_provider)
        strategy_success = await test_contextual_strategy_with_xai(llm_provider)
        enhanced_chunk_success = await test_create_enhanced_chunk_with_xai(llm_provider)
    else:
        print("\nâš ï¸  è·³è¿‡XAI LLMç›¸å…³æµ‹è¯• - LLMæä¾›è€…ä¸å¯ç”¨")
        description_success = False
        strategy_success = False
        enhanced_chunk_success = False
    
    # æ€»ç»“
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"âœ… XAI LLMæä¾›è€…: {'æˆåŠŸ' if llm_provider else 'å¤±è´¥'}")
    print(f"âœ… Promptç®¡ç†å™¨: {'æˆåŠŸ' if prompt_success else 'å¤±è´¥'}")
    print(f"âœ… XAI Contextual Description: {'æˆåŠŸ' if description_success else 'å¤±è´¥/è·³è¿‡'}")
    print(f"âœ… XAI ç­–ç•¥é›†æˆ: {'æˆåŠŸ' if strategy_success else 'å¤±è´¥/è·³è¿‡'}")
    print(f"âœ… XAI Enhanced Chunk: {'æˆåŠŸ' if enhanced_chunk_success else 'å¤±è´¥/è·³è¿‡'}")
    
    all_success = all([
        llm_provider is not None,
        prompt_success,
        description_success,
        strategy_success,
        enhanced_chunk_success
    ])
    
    if all_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼XAI LLM Contextual ChunkingåŠŸèƒ½å®Œå…¨æ­£å¸¸ã€‚")
        print("ğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•éªŒè¯contextual chunksæ˜¯å¦æ­£ç¡®ç”Ÿæˆï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥é…ç½®æˆ–ç½‘ç»œè¿æ¥ã€‚")


if __name__ == "__main__":
    asyncio.run(main())
