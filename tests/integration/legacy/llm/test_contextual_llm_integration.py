#!/usr/bin/env python3
"""
Test LLM API calling functionality, especially contextual chunking related features
Using XAI's grok-3-mini model
"""

import asyncio
import os
import sys
from pathlib import Path

# Add src path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "src"))

from memfuse_core.rag.chunk.base import ChunkData
from memfuse_core.rag.chunk.contextual import ContextualChunkStrategy
from memfuse_core.llm.base import LLMRequest, LLMResponse
from memfuse_core.llm.providers.openai import OpenAIProvider
from memfuse_core.llm.prompts.manager import get_prompt_manager


async def test_xai_llm_provider():
    """Test XAI LLM provider functionality"""
    print("ğŸ§  Testing XAI LLM provider functionality...")

    try:
        # Configuration - use XAI settings
        xai_api_key = os.getenv('XAI_API_KEY') or os.getenv('OPENAI_API_KEY')
        xai_base_url = os.getenv('XAI_API_URL') or os.getenv('OPENAI_BASE_URL')

        if not xai_api_key:
            print("âŒ XAI_API_KEY or OPENAI_API_KEY environment variable not found")
            return None

        print(f"ğŸ”‘ Using API Key: {xai_api_key[:10]}...")
        print(f"ğŸŒ Using Base URL: {xai_base_url}")

        config = {
            'api_key': xai_api_key,
            'base_url': xai_base_url,
            'model': 'grok-3-mini'
        }

        # Create OpenAI provider (compatible with XAI)
        provider = OpenAIProvider(config)

        # Test simple call
        request = LLMRequest(
            messages=[{"role": "user", "content": "Hello, respond with exactly 'LLM working!'"}],
            model="grok-3-mini",
            max_tokens=50,
            temperature=0.3
        )

        print("ğŸ“¡ Sending test request...")
        response = await provider.generate(request)

        if response.success:
            print(f"âœ… XAI LLM call successful: {response.content}")
            print(f"   Model: {response.model}")
            print(f"   Token usage: {response.usage.total_tokens}")
            return provider
        else:
            print(f"âŒ XAI LLM call failed: {response.error}")
            return None

    except Exception as e:
        print(f"âŒ XAI LLM initialization failed: {e}")
        return None


async def test_contextual_description_with_xai(llm_provider):
    """æµ‹è¯•ä½¿ç”¨XAIçš„contextual descriptionç”Ÿæˆ"""
    print("\nğŸ” æµ‹è¯•XAI Contextual Descriptionç”Ÿæˆ...")
    
    if not llm_provider:
        print("âŒ æ— æ³•æµ‹è¯• - LLMæä¾›è€…ä¸å¯ç”¨")
        return False
    
    try:
        # Create test data
        context_chunks = [
            ChunkData(
                content="[ASSISTANT]: Hi! How are you doing tonight?\n[USER]: I'm doing great. Just relaxing with my two dogs.",
                metadata={"chunk_id": "chunk_0", "session_id": "test"}
            ),
            ChunkData(
                content="[ASSISTANT]: Great. In my spare time I do volunteer work.\n[USER]: That's neat. What kind of volunteer work do you do?",
                metadata={"chunk_id": "chunk_1", "session_id": "test"}
            )
        ]
        
        current_chunk = "[ASSISTANT]: I work in a homeless shelter in my town.\n[USER]: Good for you. Do you like vintage cars? I've two older mustangs."
        
        print(f"ğŸ“„ å½“å‰chunk: {current_chunk[:50]}...")
        print(f"ğŸ”— ä¸Šä¸‹æ–‡chunks: {len(context_chunks)}")
        
        # ä½¿ç”¨promptç®¡ç†å™¨ç”Ÿæˆprompt
        prompt_manager = get_prompt_manager()
        
        past_messages = "\n\n--- Previous Chunk ---\n\n".join([
            chunk.content for chunk in context_chunks
        ])
        
        prompt = prompt_manager.get_prompt(
            "contextual_chunking",
            past_messages=past_messages,
            current_messages=current_chunk,
            chunk_content=current_chunk
        )
        
        print(f"ğŸ“ ç”Ÿæˆçš„prompté•¿åº¦: {len(prompt)}")
        
        # åˆ›å»ºLLMè¯·æ±‚ - ä½¿ç”¨grok-3-mini
        request = LLMRequest(
            messages=[{"role": "user", "content": prompt}],
            model="grok-3-mini",
            max_tokens=150,
            temperature=0.3
        )
        
        print("ğŸ“¡ å‘é€contextual descriptionè¯·æ±‚...")
        # ç”Ÿæˆå“åº”
        response = await llm_provider.generate(request)
        
        if response.success:
            print(f"âœ… XAI Contextual descriptionç”ŸæˆæˆåŠŸ:")
            print(f"   å†…å®¹: {response.content}")
            print(f"   Tokenä½¿ç”¨: {response.usage.total_tokens}")
            return True
        else:
            print(f"âŒ XAI Contextual descriptionç”Ÿæˆå¤±è´¥: {response.error}")
            return False
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
        return False


async def test_contextual_strategy_with_xai(llm_provider):
    """Test ContextualChunkStrategy integration with XAI"""
    print("\nğŸš€ Testing ContextualChunkStrategy integration with XAI...")

    if not llm_provider:
        print("âŒ Unable to test - LLM provider unavailable")
        return False

    try:
        # Create strategy instance - using grok-3-mini
        strategy = ContextualChunkStrategy(
            enable_contextual=True,
            context_window_size=2,
            llm_provider=llm_provider,
            gpt_model="grok-3-mini"
        )
        
        print(f"âœ… Strategy created successfully:")
        print(f"   enable_contextual: {strategy.enable_contextual}")
        print(f"   context_window_size: {strategy.context_window_size}")
        print(f"   llm_provider: {strategy.llm_provider is not None}")
        print(f"   gpt_model: {strategy.gpt_model}")

        # Test contextual description generation method
        context_chunks = [
            ChunkData(
                content="[ASSISTANT]: Hi! How are you doing tonight?\n[USER]: I'm doing great. Just relaxing with my two dogs.",
                metadata={"chunk_id": "chunk_0", "session_id": "test"}
            )
        ]
        
        current_chunk = "[ASSISTANT]: Great. In my spare time I do volunteer work.\n[USER]: That's neat. What kind of volunteer work do you do?"
        
        print(f"\nğŸ“„ Testing _generate_contextual_description method...")
        print("ğŸ“¡ Calling XAI API...")

        description = await strategy._generate_contextual_description(
            current_chunk, context_chunks
        )

        print(f"âœ… XAI method call successful:")
        print(f"   Description: {description}")

        return True

    except Exception as e:
        print(f"âŒ Strategy test failed: {e}")
        return False


async def test_create_enhanced_chunk_with_xai(llm_provider):
    """Test _create_enhanced_chunk_async method using XAI"""
    print("\nğŸ”§ Testing XAI _create_enhanced_chunk_async method...")

    if not llm_provider:
        print("âŒ Cannot test - LLM provider unavailable")
        return False

    try:
        # Create strategy instance
        strategy = ContextualChunkStrategy(
            enable_contextual=True,
            context_window_size=2,
            llm_provider=llm_provider,
            gpt_model="grok-3-mini"
        )
        
        # Create test data
        previous_chunks = [
            ChunkData(
                content="[ASSISTANT]: Hi! How are you doing tonight?\n[USER]: I'm doing great. Just relaxing with my two dogs.",
                metadata={"chunk_id": "chunk_0", "session_id": "test"}
            )
        ]

        chunk_content = "[ASSISTANT]: Great. In my spare time I do volunteer work.\n[USER]: That's neat. What kind of volunteer work do you do?"

        print(f"ğŸ“„ Testing chunk content: {chunk_content[:50]}...")
        print(f"ğŸ”— Historical chunks: {len(previous_chunks)}")
        print("ğŸ“¡ Calling XAI API to generate enhanced chunk...")

        # Call method
        enhanced_chunk = await strategy._create_enhanced_chunk_async(
            chunk_content, 0, previous_chunks, "test_session"
        )
        
        print(f"âœ… XAI Enhanced chunkåˆ›å»ºæˆåŠŸ:")
        print(f"   has_context: {enhanced_chunk.metadata.get('has_context')}")
        print(f"   gpt_enhanced: {enhanced_chunk.metadata.get('gpt_enhanced')}")
        print(f"   context_window_size: {enhanced_chunk.metadata.get('context_window_size')}")
        print(f"   context_chunk_ids: {enhanced_chunk.metadata.get('context_chunk_ids')}")
        
        if enhanced_chunk.metadata.get('contextual_description'):
            desc = enhanced_chunk.metadata['contextual_description']
            print(f"   contextual_description: {desc}")
        
        return True
        
    except Exception as e:
        print(f"âŒ XAI Enhanced chunkåˆ›å»ºå¤±è´¥: {e}")
        return False


async def main():
    """Main test function"""
    print("ğŸ§ª Starting XAI LLM Contextual Chunking comprehensive test\n")
    print(f"ğŸ“ Current working directory: {os.getcwd()}")
    print(f"ğŸ” Checking environment variables...")
    print(f"   XAI_API_KEY: {'Set' if os.getenv('XAI_API_KEY') else 'Not set'}")
    print(f"   OPENAI_API_KEY: {'Set' if os.getenv('OPENAI_API_KEY') else 'Not set'}")
    print(f"   XAI_API_URL: {os.getenv('XAI_API_URL', 'Not set')}")
    print()

    # 1. Test XAI LLM provider
    llm_provider = await test_xai_llm_provider()

    # 2. Test prompt manager
    try:
        prompt_manager = get_prompt_manager()
        prompt_success = True
        print("âœ… Prompt manager initialization successful")
    except Exception as e:
        prompt_success = False
        print(f"âŒ Prompt manager initialization failed: {e}")

    # 3. Test XAI related functionality
    if llm_provider:
        description_success = await test_contextual_description_with_xai(llm_provider)
        strategy_success = await test_contextual_strategy_with_xai(llm_provider)
        enhanced_chunk_success = await test_create_enhanced_chunk_with_xai(llm_provider)
    else:
        print("\nâš ï¸  Skipping XAI LLM related tests - LLM provider unavailable")
        description_success = False
        strategy_success = False
        enhanced_chunk_success = False

    # Summary
    print("\nğŸ“Š Test Summary:")
    print(f"âœ… XAI LLM Provider: {'Success' if llm_provider else 'Failed'}")
    print(f"âœ… Prompt Manager: {'Success' if prompt_success else 'Failed'}")
    print(f"âœ… XAI Contextual Description: {'Success' if description_success else 'Failed/Skipped'}")
    print(f"âœ… XAI Strategy Integration: {'Success' if strategy_success else 'Failed/Skipped'}")
    print(f"âœ… XAI Enhanced Chunk: {'Success' if enhanced_chunk_success else 'Failed/Skipped'}")

    all_success = all([
        llm_provider is not None,
        prompt_success,
        description_success,
        strategy_success,
        enhanced_chunk_success
    ])

    if all_success:
        print("\nğŸ‰ All tests passed! XAI LLM Contextual Chunking functionality is fully operational.")
        print("ğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•éªŒè¯contextual chunksæ˜¯å¦æ­£ç¡®ç”Ÿæˆï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥é…ç½®æˆ–ç½‘ç»œè¿æ¥ã€‚")


if __name__ == "__main__":
    asyncio.run(main())
